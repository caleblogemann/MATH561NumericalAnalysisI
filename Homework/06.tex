\documentclass[11pt]{article}
\usepackage[letterpaper, margin = .75in]{geometry}
\usepackage{MATH561}
\usepackage{SetTheory}
\usepackage{Derivative}
\usepackage{Vector}
\usepackage{Complex}
\usepackage{amsbsy}
\allowdisplaybreaks


\begin{document}
\noindent \textbf{\Large{Caleb Logemann \\
MATH 561 Numerical Analysis I \\
Final Assignment
}}

\begin{enumerate}
    \item % #1 Done
        Let $x_1, x_2, \ldots, x_n$, for $n > 1$, be machine numbers.
        Their product can be computed by the alogirithm
        \begin{align*}
            p_1 &= x_1 \\
            p_k &= fl(x_k p_{k-1}), k = 2, 3, \ldots, n
        \end{align*}
        \begin{enumerate}
            \item[(a)]
                Find an upper bound for the relative error in terms of the
                machine precision $eps$ and $n$.

                The relative error is given by
                \[
                    \frac{p_n - x_1x_2\cdots x_n}{x_1x_2\cdots x_n}
                \]

                First consider $p_k$.
                \begin{align*}
                    p_k &= fl(x_k p_{k-1}) \\
                        &= x_k p_{k-1} (1 + \epsilon_k)
                    \intertext{Where $\abs{\epsilon_k} < eps$, for $k = 1, \cdots, n$}
                    &< x_k p_{k-1} (1 + eps)
                    \intertext{Applying this recursively to $p_n$, we see that}
                    p_n &< x_n p_{n-1} (1 + eps) \\
                        &< x_n x_{n-1} p_{n-2} (1 + eps)^2 \\
                        &< x_n x_{n-1} x_{n-2} p_{n-3} (1 + eps)^3 \\
                        &\vdots \\
                        &< x_n x_{n-1} \cdots x_1 (1 + eps)^{n-1}
                \end{align*}

                Therefore the relative error can be bounded as follows
                \begin{align*}
                    E &= \abs{\frac{p_n - x_1x_2\cdots x_n}{x_1x_2\cdots x_n}} \\
                    &< \abs{\frac{x_n x_{n-1} \cdots x_1 (1 + eps)^{n-1} - x_1x_2\cdots x_n}{x_1x_2\cdots x_n}} \\
                    &= \abs{\frac{x_1x_2\cdots x_n\p{(1 + eps)^{n-1} - 1}}{x_1x_2\cdots x_n}} \\
                    &= (1 + eps)^{n-1} - 1 \\
                \end{align*}
                Therefore the upper bound for the relative error is
                $E < (1 + eps)^{n-1} - 1$.

            \item[(b)]
                For any integer $r$ that satisfies $r \times eps < \frac{1}{10}$,
                show that
                \[
                    (1 + eps)^r - 1 < 1.06 \times r \times eps
                \]
                Hence for $n$ not too large, simplify the answer given in (a).

                % Use the Binomial Theorem
                Using the Binomial Thereom, $(1 + eps)^r$ can be expanded.
                \begin{align*}
                    (1 + eps)^r - 1 &= \sum{i = 0}{r}{\binom{r}{i} 1^{r - i} eps^i} - 1 \\
                    &= \sum{i = 1}{r}{\binom{r}{i} eps^i} \\
                    &= r \cdot eps + \binom{r}{2} eps^2 + \binom{r}{3} eps^3 + \cdots + eps^r \\
                    &= r \cdot eps + \frac{r(r-1)}{2} eps^2 + \frac{r(r-1)(r-2)}{3!} eps^3 + \cdots + eps^r \\
                    &= r \cdot eps \p{1 + \frac{r-1}{2} eps + \frac{(r-1)(r-2)}{3!} eps^2 + \cdots + \frac{(r-1)(r-2)\cdots(1)}{r!} eps^{r-1}} \\
                    \intertext{Since $r \times eps < \frac{1}{10}$, $(r-i) eps < \frac{1}{10}$ for any $0 < i < r$}
                    &< r \cdot eps \p{1 + \frac{1}{2} \frac{1}{10} + \frac{1}{3!} \p{\frac{1}{10}}^2 + \cdots + \frac{1}{r!} \p{\frac{1}{10}}^{r-1}} \\
                    &= r \cdot eps \sum{k=0}{r-1}{\frac{1}{k!} \p{\frac{1}{10}}^{k-1}} \\
                    &= r \cdot eps \cdot 10 \sum{k=1}{r-1}{\frac{1}{k!} \p{\frac{1}{10}}^{k}}
                    \intertext{This expression is certainly less than extending
                        the sum to infinity because all of the terms are postive.
                        Also this sum is the Taylor series for $e^x - 1$.}
                    &< r \cdot eps \cdot 10 \sum{k=1}{\infty}{\frac{1}{k!} \p{\frac{1}{10}}^{k}} \\
                    &= r \cdot eps \cdot 10 \p{e^{1/10} - 1} \\
                    &\approx 1.05171 r \cdot eps \\
                    &< 1.06 r \cdot eps
                \end{align*}
                This result can now be used to simplify the result of part (a).
                Now if $n$ is not too large, then $\abs{E} < 1.06 (n-1)eps$.
        \end{enumerate}

    \item % #2 Done
        \begin{enumerate}
            \item[(a)] % Done
                Determine
                \[
                    \min* \max*_{a \le x \le b} \abs{a_0 x^n + a_1 x^{n-1} + \cdots + a_n}
                \]
                for $n \ge 1$ where the minimum is taken over the coefficients
                $a_0, a_1, \ldots, a_n$ with $a_0 \neq 0$.

                First lets apply a linear transformation from the interval
                $[a, b]$ to $[-1, 1]$, by letting
                $x = \frac{b - a}{2} t + \frac{b + a}{2}$.
                This is then equivalent to
                \begin{align*}
                    &\min* \max*_{-1 \le t \le 1} \abs{a_0 \p{\frac{b - a}{2} t + \frac{b + a}{2}} + a_1 \p{\frac{b - a}{2} t + \frac{b + a}{2}}^{n-1} + \cdots + a_n} \\
                    &= \min* \max*_{-1 \le t \le 1} \abs{a_0 \p{\frac{b - a}{2}}^n t^n + b_1 t^{n-1} + \cdots + b_n} \\
                    &= \abs{a_0} \p{\frac{b - a}{2}}^n \min* \max*{-1 \le t \le 1} \abs{t^n + b_1 t^{n-1} + \cdots + b_n}
                    \intertext{From Chebychev's Theorem the monic polynomial
                        with minimum maximum value over $[-1, 1]$ is the monic
                        Chebychev polynomial}
                    &= \abs{a_0} \p{\frac{b - a}{2}}^n \max*{-1 \le t \le 1} \abs{\mathring{T}_n(x)}
                    \intertext{Also from Chebyshev's Theorem, $\max*{-1 \le t \le 1} \abs{\mathring{T}_n(x)} = \frac{1}{2^{n-1}}$}
                    &= \abs{a_0} \p{\frac{b - a}{2}}^n \frac{1}{2^{n-1}} \\
                    &= 2\abs{a_0}\p{\frac{b - a}{4}}^n
                \end{align*}
                Thus given an arbitrary choice of $a_0 \neq 0$, 
                \[
                    \min* \max*_{a \le x \le b} \abs{a_0 x^n + a_1 x^{n-1} + \cdots + a_n} = 2\abs{a_0}\p{\frac{b - a}{4}}^n
                \]

            \item[(b)] % Done
                Let $a > 1$ and $\PP_n^a = \set{p \in \PP_n | p(a) = 1}$.
                Define $\hat{p}_n \in \PP_n^a$ by $\hat{p}_n = T_n(x)/T_n(a)$,
                where $T_n(x)$ is the Chebyshev polynomial of degree $n$.
                Prove that $\norm[\infty]{\hat{p}_n} \le \norm[\infty]{p}$ for
                all $p \in \PP_n^a$.

                \begin{proof}
                    Assume to the contrary that there exists $p \in \PP_n^a$
                    such that $\norm[\infty]{p} < \norm[\infty]{\hat{p}_n}$.
                    Define the polynomial $d(x) = \hat{p}_n(x) - p(x)$.
                    Since $d$ is the difference of two degree $n$ polynomials,
                    the degree of $d$ can be at most $n$.

                    Let $\set{y_k}_{k=0}^{n}$ denote the $n + 1$ extrema points
                    for the Chebyshev polynomial $T_n(x)$, that is
                    $T_n(y_k) = (-1)^k$.
                    Obviously $\hat{p}_n$ is just a scaling of $T_n(x)$, so
                    $\norm[\infty]{\hat{p}_n} = \norm[\infty]{T_n(x)}/\abs{T_n(a)}
                    = \abs{T_n(y_k)/T_n(a)} = \abs{\hat{p}_n(y_k)}$.
                    Also since $\norm[\infty]{p} < \norm[\infty]{\hat{p}_n}$, 
                    then $\abs{p(y_k)} < \abs{\hat{p}_n(y_k)}$.

                    Now consider $d(y_k) = \hat{p}_n(y_k) - p(y_k)$.
                    Since the magnitude of $p(y_k)$ is less than the magnitude
                    of $\hat{p}_n(y_k)$, $d(y_k)$ has the same sign as
                    $\hat{p}_n(y_k)$.
                    Also as previously noted the sign of $\hat{p}_n(y_k)$
                    alternates for $k = 0, 1, \ldots, n$.
                    Therefore since $d$ is polynomial and continuous and
                    alternates sign $n + 1$ times, $d$ must have at least $n$
                    zeros.
                    Note that these occur in the interval $[-1, 1]$ as all
                    the extreme values of $T_n(x)$ are in $[-1, 1]$.
                    Now consider $d(a) =\hat{p}_n(a)  - p(a) = 1 - 1 = 0$.
                    Therefore $d$ also has a zero at $x = a$.
                    This totals $n + 1$ zeros as $a > 1$.
                    This contradicts the fact that $d$ is at most
                    a degree n polynomial.
                    Therefore our initial assumption must be incorrect, and in
                    fact $\norm[\infty]{p} \ge \norm[\infty]{\hat{p}_n(y_k)}$ for
                    all $p \in \PP_n^a$.
                \end{proof}

            \item[(c)] % Done
                Let $f$ be a positive function defined on $[a, b]$ and assume
                \begin{align*}
                    \min*_{a \le x \le b} \abs{f(x)} &= m_0 \\
                    \max*_{a \le x \le b} \abs{f^{(k)}(x)} &= M_k, k = 0, 1, 2 \ldots
                \end{align*}
                \begin{enumerate}
                    \item[(c.1)] % Done
                        Let $p_{n-1}(x)$ denote the polynomial of degree at most
                        $n - 1$ interpolating $f$ at the $n$ Chebyshev points on
                        $[a, b]$.
                        Estimate the maximum relative error
                        $r_n = \max*_{a \le x \le b} \abs{\frac{f(x) - p_{n-1}(x)}{f(x)}}$.

                        First from the error formula for an interpolating
                        polynomial we know that
                        \[
                            f(x) - p_{n-1}(x) = \frac{f^{(n)}(\xi(x))}{n!}\prod{i=1}{n}{x - x_i}
                        \]
                        where $x_i$ for $i = 1, 2, \ldots, n$ are the
                        Chebyshev nodes on $[a, b]$.
                        By transforming to the interval $[-1, 1]$, a better
                        approximation can be made.
                        Let $x = \frac{b-a}{2}t + \frac{b + a}{2}$, then
                        $x_i = \frac{b-a}{2}t_i + \frac{b + a}{2}$ where
                        $t_i$ are the Chebyshev nodes on $[-1, 1]$.
                        More specifically $t_i = \cos{\frac{2i-1}{2n}\pi}$.
                        Now we note that
                        \begin{align*}
                            \prod{i=1}{n}{x - x_i} &= \prod{i=1}{n}{\frac{b-a}{2}t + \frac{b + a}{2} - \p{\frac{b-a}{2}t_i + \frac{b + a}{2}}} \\
                            &= \prod{i=1}{n}{\frac{b-a}{2}t - \frac{b-a}{2}t_i} \\
                            &= \p{\frac{b-a}{2}}^n\prod{i=1}{n}{t-t_i}
                        \end{align*}
                        Note that $\prod{i=1}{n}{t-t_i}$ is the monic Chebyshev polynomial of
                        degree $n$, that is $\prod{i=1}{n}{t-t_i} = \mathring{T}_{n}(t)$.
                        From Chebyshev's Theorem we know that
                        $\norm[\infty]{\mathring{T}_{n}(t)} = \frac{1}{2^{n-1}}$.
                        Therefore we know that
                        \begin{align*}
                            \abs{\prod{i=1}{n}{x - x_i}} &\le \p{\frac{b-a}{2}}^n \frac{1}{2^{n-1}} \\
                            &= 2 \p{\frac{b-a}{4}}^n
                            \intertext{Also we have bound on $f^{(n)}(\xi(x))$,
                                that is $f^{(n)}(\xi(x)) \le M_n$.
                                Therefore we can bound the absolute error as}
                            \abs{f(x) - p_{n-1}(x)} &= \abs{\frac{f^{(n)}(\xi(x))}{n!}\prod{i=1}{n}{x - x_i}} \\
                            &\le 2\frac{M_n}{n!}\p{\frac{b-a}{4}}^n \\
                            \intertext{Lastly the relative error can be bounded
                                because $\abs{f(x)}$ has a lower bound, that is $\abs{f(x)} > m_0$}
                            \abs{\frac{f(x) - p_{n-1}(x)}{f(x)}} &\le 2\frac{M_n}{m_0 n!}\p{\frac{b-a}{4}}^n
                        \end{align*}
                        Thus the maximum relative error is $2\frac{M_n}{m_0 n!}\p{\frac{b-a}{4}}^n$.

                    \item[(c.2)] % Done
                        Apply the result of (c.1) to $f(x) = \ln{x}$ on
                        $I_r = [e^r, e^{r+1}]$, for an integer $r \ge 1$.
                        In particular, show that $r_n \le \alpha(r, n) c^n$,
                        where $0 < c < 1$ and $\alpha$ is slowly varying.
                        Exhibit c.

                        First we must find bounds for $f(x)$ and its derivatives.
                        \begin{align*}
                            f^{(n)}(x) &= (-1)^{n+1} (n-1)! \frac{1}{x^n}
                            \intertext{Therefore on the inteval $[e^r, e^{r+1}]$}
                            \abs{f^{(n)}(x)} &\le (n-1)! e^{-rn}
                        \end{align*}
                        Also $\abs{f(x)} < \ln{e^r} = r$ on $[e^r, e^{r+1}]$.
                        Thus $m_0 = r$ and $M_n = (n-1)! e^{-rn}$.
                        We can now construct an upper bound on the relative error
                        \begin{align*}
                            \abs{\frac{f(x) - p_{n-1}(x)}{f(x)}} &\le 2\frac{M_n}{m_0 n!}\p{\frac{b-a}{4}}^n \\
                            &= 2\frac{(n-1)! e^{-rn}}{r n!}\p{\frac{e^{r+1} - e^r}{4}}^n \\
                            &= 2\frac{e^{-rn}}{r n} \p{\frac{e^{r+1} - e^r}{4}}^n \\
                            &= \frac{2}{r n} \p{\frac{e^{-r}\p{e^{r+1} - e^r}}{4}}^n \\
                            &= \frac{2}{r n} \p{\frac{e - 1}{4}}^n \\
                        \end{align*}
                        Thus $r_n \le \alpha(r, n) c^n$, where
                        $\alpha(r, n) = \frac{2}{rn}$ and $c = \frac{e-1}{4}$.
                \end{enumerate}
        \end{enumerate}

    \item % #3 Done
        Let $f:\RR \to \RR$ be a function defined and integrable on $[-1, 1]$.
        Let $-1 = x_0 < x_1 < \cdots < x_n = 1$ be a partition of $[-1, 1]$.
        Consider the following numerical quadrature
        \[
            I(f) = \dintt{-1}{1}{f(x)}{x} \approx \sum{i=0}{n}{w_i f(x_i)} = I_n(f)
        \]
        where
        \[
            w_i = \dintt{-1}{1}{L_i(x)}{x} \text{ with }
            L_i(x) = \prod{k=0, k \neq i}{n}{\frac{x - x_k}{x_i - x_k}}
        \]
        for $i = 0, 1, 2, \ldots, n$.
        \begin{enumerate}
            \item[(a)] % Done
                Prove that if $n$ is even and the quadrature points are evenly
                spaced: $x_i = -1 + ih$ and $h = 2/n$, then the numerical
                quadrature is exact for polynomials of degree $n + 1$.

                \begin{proof}
                    First note that this numerical quadrature is equivalent to
                    $I_n(f) = \dintt{-1}{1}{p(x)}{x}$, where $p(x)$ is the unique
                    interpolating polynomial of $f$ on the points $x_i$ for
                    $i = 0, 1, \ldots, n$.
                    Thus the error of the numerical quadrature is equal to
                    the integral of the error of the interpolating polynomial.
                    \begin{align*}
                        E(f) &= \dintt{-1}{1}{\frac{f^{(n+1)}(\xi(x))}{(n+1)!}\prod{i=0}{n}{x - x_i}}{x}
                    \end{align*}
                    If $f$ is a polynomial of degree $n$ or less than
                    $f^{(n+1)}(\xi(x)) = 0$ for any $x$.
                    Thus the error is zero for $f \in \PP_n$, and so the numerical
                    quadrature is exact for polynomials of degree at most $n$.
                    If $f$ is a polynomial of degree $n+1$, then
                    $f^{(n+1)}(\xi(x))$ is constant for all $x$.
                    Thus the error becomes
                    \[
                        E(f) = \frac{f^{(n+1)}(\xi)}{(n+1)!} \dintt{-1}{1}{\prod{i=0}{n}{x - x_i}}{x}
                    \]
                    Now consider the polynomial $p(x) = \prod{i=0}{n}{x - x_i}$.
                    Note that since the set of interpolating points are spaced evenly
                    around zero, if the point $x \in \set{x_i}$, then
                    $-x \in \set{x_i}$.
                    Since $n$ is even, then $n/2$ is an integer and $x_{n/2} = 0$.
                    Also for any $k$ such that $-n/2 \le k \le n/2$, 
                    \begin{align*}
                        x_{n/2 - k} &= -1 + \p{n/2 - k}h \\
                                    &= -1 + 1 - kh \\
                                    &= -\p{1 - 1 + kh} \\
                                    &= -\p{-1 + \p{n/2 + k}h} \\
                                    &= -x_{n/2+k}
                    \end{align*}
                    Similarly $x_{i} = -x_{n-i}$.
                    Now consider $p(-x)$
                    \begin{align*}
                        p(-x) &= \prod{i=0}{n}{-x - x_i} \\
                        &= (-1)^{n+1} \prod{i=0}{n}{x + x_i}
                        \intertext{Since $n$ is even $(-1)^{n+1} = -1$}
                        &= -\prod{i=0}{n}{x + x_i}
                        \intertext{Since $x_i = -x_{n-i}$}
                        &= -\prod{i=0}{n}{x - x_{n-i}}
                        \intertext{This product is multiplying the same terms as
                            $p(x)$, so this product is equivalent to $p(x)$.}
                        &= -p(x)
                    \end{align*}
                    Therefore $p(x) = \prod{i=0}{n}{x - x_i}$ is an odd function,
                    and so the integral $\dintt{-1}{1}{p(x)}{x} = 0$.
                    Therefore $E(f) = 0$ for $f \in \PP_{n+1}$, and
                    so this numerical quadrature is exact for all polynomials
                    whose degree is at most $n+1$.
                \end{proof}

            \item[(b)] % Done
                Let $n = 2$ and let $x_0 = -1$, $x_1 = 0$, $x_2 = 1$.
                Compute $w_0$, $w_1$, and $w_2$, and explicitly write out the
                numerical quadrature formula in this case.

                First I will compute $L_i(x)$ for $i = 0, 1, 2$.
                \begin{align*}
                    L_0(x) &= \frac{x - 0}{-1 - 0} \cdot \frac{x-1}{-1-1} \\
                    &= \frac{x(x-1)}{2} \\
                    &= \frac{1}{2}\p{x^2 - x} \\
                    L_1(x) &= \frac{x - -1}{0 - -1} \cdot \frac{x - 1}{0 - 1} \\
                    &= \frac{(x + 1)(x - 1)}{-1} \\
                    &= -x^2 + 1 \\
                    L_2(x) &= \frac{x - -1}{1 - -1} \cdot \frac{x - 0}{1 - 0} \\
                    &= \frac{(x + 1)x}{2} \\
                    &= \frac{1}{2}\p{x^2 + x}
                \end{align*}
                Now the values of $w_0$, $w_1$, and $w_2$ can be found by computing
                the integrals over $[-1, 1]$ for these functions.

                \begin{align*}
                    w_0 &= \dintt{-1}{1}{L_0(x)}{x} \\
                    &= \frac{1}{2}\dintt{-1}{1}{x^2 - x}{x} \\
                    &= \frac{1}{2} \eval{\p{\frac{1}{3}x^3 - \frac{1}{2}x^2}}{x=-1}{1} \\
                    &= \frac{1}{2} \p{\p{\frac{1}{3} - \frac{1}{2}} - \p{-\frac{1}{3} - \frac{1}{2}}} \\
                    &= \frac{1}{2} \frac{2}{3} \\
                    &= \frac{1}{3} \\
                    w_1 &= \dintt{-1}{1}{L_1(x)}{x} \\
                    &= \dintt{-1}{1}{-x^2 + 1}{x} \\
                    &= \eval{-\frac{1}{3}x^3 + x}{x =-1}{1} \\
                    &= \p{-\frac{1}{3} + 1} - \p{\frac{1}{3} - 1} \\
                    &= 2 - \frac{2}{3} \\
                    &= \frac{4}{3} \\
                    w_2 &= \dintt{-1}{1}{L_2(x)}{x} \\
                    &= \frac{1}{2} \dintt{-1}{1}{x^2 + x}{x} \\
                    &= \frac{1}{2} \eval{\p{\frac{1}{3}x^3 + \frac{1}{2}x^2}}{x=-1}{1} \\
                    &= \frac{1}{2} \p{\p{\frac{1}{3} + \frac{1}{2}} - \p{-\frac{1}{3} + \frac{1}{2}}} \\
                    &= \frac{1}{2} \frac{2}{3} \\
                    &= \frac{1}{3} \\
                \end{align*}
                Thus the numerical quadrature can be written explicitly as
                \begin{align*}
                    I_n(f) &= \frac{1}{3}f(-1) + \frac{4}{3}f(0) + \frac{1}{3}f(1) \\
                    I_n(f) &= \frac{1}{3}\p{f(-1) + 4f(0) + f(1)}
                \end{align*}

            \item[(c)] % Done
                When $n = 2$ and $x_0 = -1$, $x_1 = 0$, $x_2 = 1$, what is the
                degree of precision of the numerical quadrature formula?

                We have shown in part (a) that this numerical quadrature is
                exact for polynomials of degree $n + 1 = 3$ or less.
                If the quadrature formula is not exact for polynomials
                of degree 4, then the degree of precision is 3.
                Consider $f(x) = x^4$, then
                \begin{align*}
                    E(f) &= \dintt{-1}{1}{f(x)}{x} - \frac{1}{3}\p{f(-1) + 4f(0) + f(1)} \\
                    &= \dintt{-1}{1}{x^4}{x} - \frac{1}{3}\p{1 + 4\times0 + 1} \\
                    &= \eval{\frac{1}{5}x^5}{x = -1}{1} - \frac{2}{3} \\
                    &= \frac{1}{5} - -\frac{1}{5} - \frac{2}{3} \\
                    &= \frac{2}{5} - \frac{2}{3} \\
                    &= -\frac{4}{15}
                \end{align*}
                Since the error does not equal 0, the quadrature formula is
                not exact for polynomials of degree $4$.
                Therefore the degree of precision is $3$ for this quadrature
                formula.
        \end{enumerate}

    \item % #4
        Let $a = x_0 < x_1 < \cdots < x_n = b$ be a partition of $[a, b]$.
        Consider a function $f \in C^{\infty}[a, b]$.
        \begin{enumerate}
            \item[(a)] % Done
                Define what it means for a function $S$ to be a linear spline
                that interpolates $f$ at all the points $x_i$ for
                $i = 0, 1, \ldots, n$.
                Give a formula for $S$ in terms of the point values of $f$.

                In order to define the linear spline, I will first define a set of
                linear basis functions.
                Let $B_i$ for $i = 1, 2, \ldots, n-1$ be defined on $[a, b]$ as follows.
                \begin{align*}
                    B_i(x) = 
                    \begin{cases}
                        \frac{x - x_{i-1}}{x_i - x_{i-1}} & x_{i-1} \le x \le x_i \\
                        \frac{x_{i+1} - x}{x_{i+1} - x_i} & x_i < x \le x_{i+1} \\
                        0 & \text{otherwise}
                    \end{cases}
                \end{align*}
                Also let $B_1$ and $B_n$ be defined as follows
                \begin{align*}
                    B_1(x) &=
                    \begin{cases}
                        \frac{x - x_{n-1}}{x_n - x_{n-1}}& a = x_{0} \le x \le x_1 \\
                        0 & x > x_1
                    \end{cases} \\
                    B_n(x) &=
                    \begin{cases}
                        \frac{x_1 - x}{x_1 - x_0}  & x_{n-1} \le x \le x_n = b \\
                        0 & x < x_{n-1}
                    \end{cases}
                \end{align*}

                A linear spline on $[a, b]$ that interpolates $f$ on the
                partition $\set{x_i}_{i=0}^{n}$ is a function $S(x)$ that is a
                linear combination of the basis functions $B_i$ such that
                $S(x_i) = f(x_i)$ for $i = 0, 1, \ldots, n$.

                Thus a formula for $S(x)$ could be written as
                $S(x) = \sum{i=0}{n}{f(x_i)B_i(x)}$.

            \item[(b)]
                Let $h = \max*_{0 \le i \le n-1}(x_{i+1} - x_i)$.
                Derive an upper bound on $\abs{f(x) - S(x)}$ on $x \in [a, b]$.
                Use this to prove that $\lim{h \to 0}{\abs{f(x) - S(x)}} = 0$
                for $x \in [a, b]$ and state the rate of convergence.



            \item[(c)] % Done
                Define what it means for $S$ to be a clamped cubic spline tat
                interpolates $f$ at all the points $x_i$, for $i = 0, 1, \ldots, n$.

                A function $S(x)$ is a clamped cubic spline that interpolates $f$
                at the points $x_i$ for $i = 0, 1, \ldots, n$ if $S(x)$ is piecewise
                cubic.
                $S(x)$ can be expressed as $a_3 x^3 + a_2 x^2 + a_1 x + a_0$ on each
                interval $[x_i, x_{i+1}]$ for $i = 0, 1, \ldots, n-1$.
                I will denote each of these pieces as $S_i(x)$.
                Furthermore $S(x)$ must satisfy some other properties.
                $S(x)$ must be match the function values of $f$ at each $x_i$,
                that is $S_i(x_i) = f(x_i)$ and $S_i(x_{i+1}) = f(x_{i+1})$
                for $i = 0, 1, \ldots, n-1$.
                Furthermore $S(x)$ must have a continuous
                first and second derivative, this can be written at
                $S_i^{(k)}(x_{i+1}) = S_{i+1}^{(k)}(x_{i+1})$ for $k = 1, 2$ and
                $i = 0, 1, \ldots, n - 2$.
                Lastly for $S(x)$ to be clamped we require that the first
                derivatives of $S(x)$ match the derivatives of $f$ at the
                endpoints, that is $S_0'(x_0) = f'(x_0)$ and
                $S_{n-1}(x_n) = f'(x_n)$.
                These conditions provide $4n$ equations for the $4n$ coefficients
                of the cubic pieces.
                Thus these conditions fully define the clamped cubic spline.
        \end{enumerate}

    \item % #5 Done
        \begin{enumerate}
            \item[(a)] % Done
                Prove the following theorem: Consider the system of initial
                value problems:
                \[
                    \v{y}' = f(\v{y})
                \]
                and apply it to the forward Euler method:
                \[
                    \v{u}_{n+1} = F(\v{u}_n) = \v{u}_n + h f(\v{u}_n)
                \]
                Then
                \begin{itemize}
                    \item
                        $\pmb{\alpha}$ is a fixed point of the Euler method,
                        that is $F(\pmb{\alpha}) = \pmb{\alpha}$ if and only if
                        $\pmb{\alpha}$ is a fixed point of the initial value
                        problem, that is $f(\pmb{\alpha}) = \v{0}$.

                    \item 
                        If $\pmb{\alpha}$ is a linearly stable fixed point of the
                        initial value problem (i.e. all the eigenvalues of the matrix
                        $\pd{f}{\v{y}}(\pmb{\alpha})$ have negative real parts) and if
                        $\abs{1 + h \lambda_p} < 1$ for each eigenvalue
                        $\lambda_p$ of $\pd{f}{\v{y}}(\pmb{\alpha})$, then
                        $\pmb{\alpha}$ is also a linearly stable fixed point of
                        the Euler method.
                \end{itemize}

                \begin{proof}
                    To prove the first point, suppose $\v{\alpha}$ is a fixed point
                    of the Euler method, then
                    \begin{align*}
                        F(\pmb{\alpha}) &= \pmb{\alpha} \\
                        \pmb{\alpha} + f(\pmb{\alpha}) =  \pmb{\alpha} \\
                        f(\pmb{\alpha}) &= \v{0}
                    \end{align*}
                    Thus $\pmb{\alpha}$ is also a fixed point the initial value
                    problem.
                    Reversing this procedure shows that is $\pmb{\alpha}$ is
                    a fixed point of the initial value problem it is also
                    a fixed point of the Euler method.
                    Therefore $\pmb{\alpha}$ is a fixed point for the Euler method
                    if and only if $\pmb{\alpha}$ is a fixed point for the initial
                    value problem.

                    To prove the second point, suppose $\pmb{\alpha}$ is a
                    linearly fixed point of the initial value problem.
                    This implies that $f(\pmb{\alpha}) = 0$ and the
                    eigenvalues of $\pd{f}{\v{y}}(\pmb{\alpha})$ have negative
                    real parts.
                    In order for $\pmb{\alpha}$ to be a linearly stable fixed
                    point of the Euler method $F$, then the magnitude of the
                    eigenvalues of $\pd{F}{\v{u}}(\pmb{\alpha})$ must be less
                    than 1.
                    Note that $\pd{F}{\v{u}}(\pmb{\alpha}) = I + h\pd{f}{\v{u}}(\pmb{\alpha})$,
                    where $I$ is the identity matrix.
                    Suppose $\lambda_p$ is an eigenvalue of
                    $\pd{f}{\v{y}}(\pmb{\alpha})$ with eigenvector $\v{x}$.
                    Now consider $\pd{F}{\v{u}}(\pmb{\alpha}) \v{x}$.
                    \begin{align*}
                        \pd{F}{\v{u}}(\pmb{\alpha}) \v{x} &= \p{I + h\pd{f}{\v{u}}(\pmb{\alpha})} \v{x} \\
                        &= \v{x} + h\pd{f}{\v{y}}(\pmb{\alpha}) \v{x}
                        \intertext{Since $\v{x}$ is an eigenvector of $\pd{f}{\v{u}}(\pmb{\alpha})$}
                        &= \v{x} + h \lambda_p \v{x} \\
                        &= \p{1 + h \lambda_p} \v{x}
                    \end{align*}
                    Thus $1 + h \lambda_p$ is an eigenvalue of
                    $\pd{F}{\v{u}}(\pmb{\alpha})$.
                    Therefore for all eigenvalues $\lambda_p$ of
                    $\pd{f}{\v{y}}(\pmb{\alpha})$, $1 + h \lambda_p$ is
                    an eigenvalue of $\pd{F}{\v{u}}(\pmb{\alpha})$.
                    Also any eigenvalue of $\pd{F}{\v{u}}(\pmb{\alpha})$ must
                    be of this form.
                    Now since all eigenvalues of $\pd{f}{\v{y}}(\pmb{\alpha})$
                    have negative real parts and satisfy $\abs{1 + h \lambda_p} < 1$,
                    the eigenvalues $\lambda_F$ of $\pd{F}{\v{u}}(\pmb{\alpha})$
                    must satisfy $\abs{\lambda_F} = \abs{1 + h \lambda_p} < 1$.
                    Therefore $\pmb{\alpha}$ is a linearly stable fixed point
                    of the Euler method.
                \end{proof}

            \item[(b)] % Done
                The fixed points of the Logistic growth equation
                \[
                    y' = f(y) = 2y(1 - y)
                \]
                are $y = 0$ (unstable since $f'(0) = 2$) and $y = 1$ (stable
                since $f'(1) = -2$).
                Apply the Euler method to this equation and find and classify all
                fixed points of the Euler method as a function of the time step
                parameter $h$.

                According to the previous theorem the fixed points of the Euler
                method must also be fixed points of the initial value problems.
                We can see that $f(y) = 0$ for $y = 0$ and $y = 1$.
                Therefore the fixed points of the Euler method are $y = 0$ and
                $y = 1$.
                If the eigenvalues, $\lambda_p$, of $\pd{f}{\v{y}}(\v{\alpha})$ are negative and
                satisfy $\abs{1 + h\lambda_p} < 1$, then the fixed point
                $\alpha$ of the Euler method is linearly stable.
                In this one-dimensional case, the eigenvalues of
                $\pd{f}{\v{y}}(\v{\alpha})$ are simply $f'(\alpha)$.
                The derivative of $f$ is $f'(y) = 2(1-y) - 2y$.
                For $\alpha = 0$, $f'(0) = 2$, therefore this fixed point is
                unstable in both the initial value problem and the Euler method,
                for all values of $h$.
                For $\alpha = 1$, $f'(1) = -2$, and if $\abs{1 + -2h} < 1$, then
                this fixed point will be linearly stable for the Euler method.
                If $0 < h < 1$, then $\abs{1 - 2h} < 1$.
                Therefore this fixed point is linearly stable for $h \in (0, 1)$.
        \end{enumerate}
\end{enumerate}
\end{document}
