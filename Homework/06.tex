\documentclass[11pt]{article}
\usepackage[letterpaper, margin = .75in]{geometry}
\usepackage{MATH561}
\usepackage{SetTheory}
\usepackage{Derivative}
\usepackage{Vector}
\usepackage{Complex}
\allowdisplaybreaks


\begin{document}
\noindent \textbf{\Large{Caleb Logemann \\
MATH 561 Numerical Analysis I \\
Final Assignment
}}

\begin{enumerate}
    \item % #1 Done
        Let $x_1, x_2, \ldots, x_n$, for $n > 1$, be machine numbers.
        Their product can be computed by the alogirithm
        \begin{align*}
            p_1 &= x_1 \\
            p_k &= fl(x_k p_{k-1}), k = 2, 3, \ldots, n
        \end{align*}
        \begin{enumerate}
            \item[(a)]
                Find an upper bound for the relative error in terms of the
                machine precision $eps$ and $n$.

                The relative error is given by
                \[
                    \frac{p_n - x_1x_2\cdots x_n}{x_1x_2\cdots x_n}
                \]

                First consider $p_k$.
                \begin{align*}
                    p_k &= fl(x_k p_{k-1}) \\
                        &= x_k p_{k-1} (1 + \epsilon_k)
                    \intertext{Where $\abs{\epsilon_k} < eps$, for $k = 1, \cdots, n$}
                    &< x_k p_{k-1} (1 + eps)
                    \intertext{Applying this recursively to $p_n$, we see that}
                    p_n &< x_n p_{n-1} (1 + eps) \\
                        &< x_n x_{n-1} p_{n-2} (1 + eps)^2 \\
                        &< x_n x_{n-1} x_{n-2} p_{n-3} (1 + eps)^3 \\
                        &\vdots \\
                        &< x_n x_{n-1} \cdots x_1 (1 + eps)^{n-1}
                \end{align*}

                Therefore the relative error can be bounded as follows
                \begin{align*}
                    E &= \abs{\frac{p_n - x_1x_2\cdots x_n}{x_1x_2\cdots x_n}} \\
                    &< \abs{\frac{x_n x_{n-1} \cdots x_1 (1 + eps)^{n-1} - x_1x_2\cdots x_n}{x_1x_2\cdots x_n}} \\
                    &= \abs{\frac{x_1x_2\cdots x_n\p{(1 + eps)^{n-1} - 1}}{x_1x_2\cdots x_n}} \\
                    &= (1 + eps)^{n-1} - 1 \\
                \end{align*}
                Therefore the upper bound for the relative error is
                $E < (1 + eps)^{n-1} - 1$.

            \item[(b)]
                For any integer $r$ that satisfies $r \times eps < \frac{1}{10}$,
                show that
                \[
                    (1 + eps)^r - 1 < 1.06 \times r \times eps
                \]
                Hence for $n$ not too large, simplify the answer given in (a).

                % Use the Binomial Theorem
                Using the Binomial Thereom, $(1 + eps)^r$ can be expanded.
                \begin{align*}
                    (1 + eps)^r - 1 &= \sum{i = 0}{r}{\binom{r}{i} 1^{r - i} eps^i} - 1 \\
                    &= \sum{i = 1}{r}{\binom{r}{i} eps^i} \\
                    &= r \cdot eps + \binom{r}{2} eps^2 + \binom{r}{3} eps^3 + \cdots + eps^r \\
                    &= r \cdot eps + \frac{r(r-1)}{2} eps^2 + \frac{r(r-1)(r-2)}{3!} eps^3 + \cdots + eps^r \\
                    &= r \cdot eps \p{1 + \frac{r-1}{2} eps + \frac{(r-1)(r-2)}{3!} eps^2 + \cdots + \frac{(r-1)(r-2)\cdots(1)}{r!} eps^{r-1}} \\
                    \intertext{Since $r \times eps < \frac{1}{10}$, $(r-i) eps < \frac{1}{10}$ for any $0 < i < r$}
                    &< r \cdot eps \p{1 + \frac{1}{2} \frac{1}{10} + \frac{1}{3!} \p{\frac{1}{10}}^2 + \cdots + \frac{1}{r!} \p{\frac{1}{10}}^{r-1}} \\
                    &= r \cdot eps \sum{k=0}{r-1}{\frac{1}{k!} \p{\frac{1}{10}}^{k-1}} \\
                    &= r \cdot eps \cdot 10 \sum{k=1}{r-1}{\frac{1}{k!} \p{\frac{1}{10}}^{k}}
                    \intertext{This expression is certainly less than extending
                        the sum to infinity because all of the terms are postive.
                        Also this sum is the Taylor series for $e^x - 1$.}
                    &< r \cdot eps \cdot 10 \sum{k=1}{\infty}{\frac{1}{k!} \p{\frac{1}{10}}^{k}} \\
                    &= r \cdot eps \cdot 10 \p{e^{1/10} - 1} \\
                    &\approx 1.05171 r \cdot eps \\
                    &< 1.06 r \cdot eps
                \end{align*}
                This result can now be used to simplify the result of part (a).
                Now if $n$ is not too large, then $\abs{E} < 1.06 (n-1)eps$.
        \end{enumerate}

    \item % #2
        \begin{enumerate}
            \item[(a)] % Done
                Determine
                \[
                    \min* \max*_{a \le x \le b} \abs{a_0 x^n + a_1 x^{n-1} + \cdots + a_n}
                \]
                for $n \ge 1$ where the minimum is taken over the coefficients
                $a_0, a_1, \ldots, a_n$ with $a_0 \neq 0$.

                First lets apply a linear transformation from the interval
                $[a, b]$ to $[-1, 1]$, by letting
                $x = \frac{b - a}{2} t + \frac{b + a}{2}$.
                This is then equivalent to
                \begin{align*}
                    &\min* \max*_{-1 \le t \le 1} \abs{a_0 \p{\frac{b - a}{2} t + \frac{b + a}{2}} + a_1 \p{\frac{b - a}{2} t + \frac{b + a}{2}}^{n-1} + \cdots + a_n} \\
                    &= \min* \max*_{-1 \le t \le 1} \abs{a_0 \p{\frac{b - a}{2}}^n t^n + b_1 t^{n-1} + \cdots + b_n} \\
                    &= \abs{a_0} \p{\frac{b - a}{2}}^n \min* \max*{-1 \le t \le 1} \abs{t^n + b_1 t^{n-1} + \cdots + b_n}
                    \intertext{From Chebychev's Theorem the monic polynomial
                        with minimum maximum value over $[-1, 1]$ is the monic
                        Chebychev polynomial}
                    &= \abs{a_0} \p{\frac{b - a}{2}}^n \max*{-1 \le t \le 1} \abs{\mathring{T}_n(x)}
                    \intertext{Also from Chebyshev's Theorem, $\max*{-1 \le t \le 1} \abs{\mathring{T}_n(x)} = \frac{1}{2^{n-1}}$}
                    &= \abs{a_0} \p{\frac{b - a}{2}}^n \frac{1}{2^{n-1}} \\
                    &= 2\abs{a_0}\p{\frac{b - a}{4}}^n
                \end{align*}
                Thus given an arbitrary choice of $a_0 \neq 0$, 
                \[
                    \min* \max*_{a \le x \le b} \abs{a_0 x^n + a_1 x^{n-1} + \cdots + a_n} = 2\abs{a_0}\p{\frac{b - a}{4}}^n
                \]

            \item[(b)] % Done
                Let $a > 1$ and $\PP_n^a = \set{p \in \PP_n | p(a) = 1}$.
                Define $\hat{p}_n \in \PP_n^a$ by $\hat{p}_n = T_n(x)/T_n(a)$,
                where $T_n(x)$ is the Chebyshev polynomial of degree $n$.
                Prove that $\norm[\infty]{\hat{p}_n} \le \norm[\infty]{p}$ for
                all $p \in \PP_n^a$.

                \begin{proof}
                    Assume to the contrary that there exists $p \in \PP_n^a$
                    such that $\norm[\infty]{p} < \norm[\infty]{\hat{p}_n}$.
                    Define the polynomial $d(x) = \hat{p}_n(x) - p(x)$.
                    Since $d$ is the difference of two degree $n$ polynomials,
                    the degree of $d$ can be at most $n$.

                    Let $\set{y_k}_{k=0}^{n}$ denote the $n + 1$ extrema points
                    for the Chebyshev polynomial $T_n(x)$, that is
                    $T_n(y_k) = (-1)^k$.
                    Obviously $\hat{p}_n$ is just a scaling of $T_n(x)$, so
                    $\norm[\infty]{\hat{p}_n} = \norm[\infty]{T_n(x)}/\abs{T_n(a)}
                    = \abs{T_n(y_k)/T_n(a)} = \abs{\hat{p}_n(y_k)}$.
                    Also since $\norm[\infty]{p} < \norm[\infty]{\hat{p}_n}$, 
                    then $\abs{p(y_k)} < \abs{\hat{p}_n(y_k)}$.

                    Now consider $d(y_k) = \hat{p}_n(y_k) - p(y_k)$.
                    Since the magnitude of $p(y_k)$ is less than the magnitude
                    of $\hat{p}_n(y_k)$, $d(y_k)$ has the same sign as
                    $\hat{p}_n(y_k)$.
                    Also as previously noted the sign of $\hat{p}_n(y_k)$
                    alternates for $k = 0, 1, \ldots, n$.
                    Therefore since $d$ is polynomial and continuous and
                    alternates sign $n + 1$ times, $d$ must have at least $n$
                    zeros.
                    Note that these occur in the interval $[-1, 1]$ as all
                    the extreme values of $T_n(x)$ are in $[-1, 1]$.
                    Now consider $d(a) =\hat{p}_n(a)  - p(a) = 1 - 1 = 0$.
                    Therefore $d$ also has a zero at $x = a$.
                    This totals $n + 1$ zeros as $a > 1$.
                    This contradicts the fact that $d$ is at most
                    a degree n polynomial.
                    Therefore our initial assumption must be incorrect, and in
                    fact $\norm[\infty]{p} \ge \norm[\infty]{\hat{p}_n(y_k)}$ for
                    all $p \in \PP_n^a$.
                \end{proof}

            \item[(c)]
                Let $f$ be a positive function defined on $[a, b]$ and assume
                \begin{align*}
                    \min*_{a \le x \le b} \abs{f(x)} &= m_0 \\
                    \max*_{a \le x \le b} \abs{f^{(k)}(x)} &= M_k, k = 0, 1, 2
                \end{align*}
                \begin{enumerate}
                    \item[(c.1)]
                        Let $p_{n-1}(x)$ denote the polynomial of degree at most
                        $n - 1$ interpolating $f$ at the $n$ Chebyshev points on
                        $[a, b]$.
                        Estimate the maximum relative error
                        $r_n = \max*_{a \le x \le b} \abs{\frac{f(x) - p_{n-1}(x)}{f(x)}}$.

                    \item[(c.2)]
                        Apply the result of (c.1) to $f(x) = \ln{x}$ on
                        $I_r = [e^r, e^{r+1}]$, for an integer $r \ge 1$.
                        In particular, show that $r_n \le \alpha(r, n) c^n$,
                        where $0 < c < 1$ and $\alpha$ is slowly varying.
                        Exhibit c.
                \end{enumerate}
        \end{enumerate}

    \item % #3
        \begin{enumerate}
            \item[(a)]
                Prove that is $n$ is even and the quadrature points are evenly
                spaced: $x_i = -1 + ih$ and $h = 2/n$, then the numerical
                quadrature is exact for polynomials of degree $n + 1$.

            \item[(b)]
                Let $n = 2$
            \item[(c)]
        \end{enumerate}

    \item % #4
        Let $a = x_0 < x_1 < \cdots < x_n = b$ be a partition of $[a, b]$.
        Consider a function $f \in C^{\infty}[a, b]$.
        \begin{enumerate}
            \item[(a)] % Done
                Define what it means for a function $S$ to be a linear spline
                that interpolates $f$ at all the points $x_i$ for
                $i = 0, 1, \ldots, n$.
                Give a formula for $S$ in terms of the point values of $f$.

                In order to define the linear spline, I will first define a set of
                linear basis functions.
                Let $B_i$ for $i = 1, 2, \ldots, n-1$ be defined on $[a, b]$ as follows.
                \begin{align*}
                    B_i(x) = 
                    \begin{cases}
                        \frac{x - x_{i-1}}{x_i - x_{i-1}} & x_{i-1} \le x \le x_i \\
                        \frac{x_{i+1} - x}{x_{i+1} - x_i} & x_i < x \le x_{i+1} \\
                        0 & \text{otherwise}
                    \end{cases}
                \end{align*}
                Also let $B_1$ and $B_n$ be defined as follows
                \begin{align*}
                    B_1(x) &=
                    \begin{cases}
                        \frac{x - x_{n-1}}{x_n - x_{n-1}}& a = x_{0} \le x \le x_1 \\
                        0 & x > x_1
                    \end{cases} \\
                    B_n(x) &=
                    \begin{cases}
                        \frac{x_1 - x}{x_1 - x_0}  & x_{n-1} \le x \le x_n = b \\
                        0 & x < x_{n-1}
                    \end{cases}
                \end{align*}

                A linear spline on $[a, b]$ that interpolates $f$ on the
                partition $\set{x_i}_{i=0}^{n}$ is a function $S(x)$ that is a
                linear combination of the basis functions $B_i$ such that
                $S(x_i) = f(x_i)$ for $i = 0, 1, \ldots, n$.

                Thus a formula for $S(x)$ could be written as
                $S(x) = \sum{i=0}{n}{f(x_i)B_i(x)}$.

            \item[(b)]
                Let $h = \max*_{0 \le i \le n-1}(x_{i+1} - x_i)$.
                Derive an upper bound on $\abs{f(x) - S(x)}$ on $x \in [a, b]$.
                Use this to prove that $\lim{h \to 0}{\abs{f(x) - S(x)}} = 0$
                for $x \in [a, b]$ and state the rate of convergence.

            \item[(c)]
                Define what it means for $S$ to be a clamped cubic spline that
                interpolates $f$ at all the points $x_i$, for $i = 0, 1, \ldots, n$.
        \end{enumerate}

    \item % #5
        \begin{enumerate}
            \item[(a)]
                Prove the following theorem: Consider the system of initial
                value problems:
                \[
                    \v{y}' = f(\v{y})
                \]
                and apply it to the forward Euler method:
                \[
                    \v{u}_{n+1} = F(\v{u}_n) = \v{u}_n + h f(\v{u}_n)
                \]
                Then
                \begin{itemize}
                    \item
                        $\v{\alpha}$ is a fixed point of the Euler method,
                        that is $F(\v{\alpha}) = \v{\alpha}$ if and only if
                        $\v{\alpha}$ is a fixed point of the initial value
                        problem, that is $f(\v{\alpha}) = \v{0}$.

                    \item 
                        If $\v{\alpha}$ is a linearly stable fixed point of the
                        initial value problem (i.e. ) and if
                        $\abs{1 + h \lambda_p} < 1$ for each eigenvalue
                        $\lambda_p$ of $\pd{f}{\v{y}}(\v{\alpha})$, then
                        $\v{\alpha}$ is also a linearly stable fixed point of
                        the Euler method.
                \end{itemize}

                \begin{proof}
                    To prove the first point, suppose $\v{\alpha}$ is a fixed point
                    of the Euler method, then
                    \begin{align*}
                        F(\v{alpha}) &= \v{\alpha} \\
                        \v{\alpha} + f(\v{\alpha}) =  \v{\alpha} \\
                        f(\v{\alpha}) &= \v{0}
                    \end{align*}
                    Thus $\v{\alpha}$ is also a fixed point the initial value
                    problem.
                    Reversing this procedure shows that is $\v{\alpha}$ is
                    a fixed point of the initial value problem it is also
                    a fixed point of the Euler method.
                    Therefore $\v{\alpha}$ is a fixed point for the Euler method
                    if and only if $\v{\alpha}$ is a fixed point for the initial
                    value problem.

                    To prove the second point, 
                \end{proof}

            \item[(b)] % Done
                The fixed points of the Logistic growth equation
                \[
                    y' = f(y) = 2y(1 - y)
                \]
                are $y = 0$ (unstable since $f'(0) = 2$) and $y = 1$ (stable
                since $f'(1) = -2$).
                Apply the Euler method to this equation and find and classify all
                fixed points of the Euler method as a function of the time step
                parameter $h$.

                According to the previous theorem the fixed points of the Euler
                method must also be fixed points of the initial value problems.
                We can see that $f(y) = 0$ for $y = 0$ and $y = 1$.
                Therefore the fixed points of the Euler method are $y = 0$ and
                $y = 1$.
                If the eigenvalues, $\lambda_p$, of $\pd{f}{\v{y}}(\v{\alpha})$ are negative and
                satisfy $\abs{1 + h\lambda_p} < 1$, then the fixed point
                $\alpha$ of the Euler method is linearly stable.
                In this one-dimensional case, the eigenvalues of
                $\pd{f}{\v{y}}(\v{\alpha})$ are simply $f'(\alpha)$.
                The derivative of $f$ is $f'(y) = 2(1-y) - 2y$.
                For $\alpha = 0$, $f'(0) = 2$, therefore this fixed point is
                unstable in both the initial value problem and the Euler method,
                for all values of $h$.
                For $\alpha = 1$, $f'(1) = -2$, and if $\abs{1 + -2h} < 1$, then
                this fixed point will be linearly stable for the Euler method.
                If $0 < h < 1$, then $\abs{1 - 2h} < 1$.
                Therefore this fixed point is linearly stable for $h \in (0, 1)$.
        \end{enumerate}
\end{enumerate}
\end{document}
